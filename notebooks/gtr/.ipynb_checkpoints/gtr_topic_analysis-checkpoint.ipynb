{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cairo\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "from itertools import combinations\n",
    "import networkx as nx\n",
    "import community #Get this package: http://perso.crans.org/aynaud/communities/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_network(g):\n",
    "    out_dir = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "    out_dir = out_dir + '/data/networks/'\n",
    "    out_file = out_dir + 'gtr_network_' + datetime.now().strftime('%Y%m%d%H%M') + '.gml'\n",
    "    try:\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "    except OSError:\n",
    "        \"Print output directory already exists. Saving network to {}\".format(out_file)\n",
    "    nx.write_gml(g, out_file)\n",
    "\n",
    "def extract_network_from_corpus(label_corpus, edge_attr=\"None\", stop_words=None):\n",
    "    \"\"\"\n",
    "    Extracts a network object from a list or series where every observation is a list of co-ocurring observations\n",
    "    (e.g. interacting agents, or co-occurring words). \n",
    "    In the network output, the labels are nodes and co-occurrences between them are edges. \n",
    "    Weight is number of co-occurrences. There is the option to consider additional edge attributes.\n",
    "\n",
    "    Inputs:\n",
    "    label_corpus: corpus of documents with label co-occurrence or node interaction.\n",
    "    edge_attr: an edge attribute associated to the interaction. It defaults to the string none (which we use\n",
    "        for control flow)\n",
    "    stop_words: labels to remove (e.g. generic terms)\n",
    "    \n",
    "    Returns a networkx object.\n",
    "    \n",
    "    \"\"\"\n",
    "    label_pairs_container =[]\n",
    "    \n",
    "    #!!! TODO Using type to control flow feels a bit hacky.\n",
    "    if type(edge_attr) != str:\n",
    "        #Container for pairs of labels\n",
    "        edge_attr_name = edge_attr.name\n",
    "    \n",
    "    #For each pair\n",
    "    for i in range(len(label_corpus)):\n",
    "        #List of tuples where every tuple is a combination of topics\n",
    "        pairs = list(combinations(label_corpus.iloc[i], 2))\n",
    "\n",
    "        #Extract these as lists of dicts capturing connections between topics, and add those to the container.\n",
    "        #NB we are sorting the tuples to make sure that we don't duplicate edges.\n",
    "        if len(pairs)>0:\n",
    "            cont = [{'e1':sorted(tup)[0],'e2':sorted(tup)[1]} for tup in pairs]\n",
    "\n",
    "            #Give each edge its corresponding attribute (if we have one)\n",
    "            if type(edge_attr) != str:\n",
    "                for d in cont:\n",
    "                    d.update({edge_attr_name: edge_attr[i]})\n",
    "\n",
    "            label_pairs_container.append(cont)\n",
    "\n",
    "    #Flatten dict list and turn into DF\n",
    "    label_pairs_df = pd.DataFrame([ds for sublist in label_pairs_container for ds in sublist])\n",
    "    \n",
    "    #Extract weights (depends on edge_attr)\n",
    "    if type(edge_attr) != str:\n",
    "        label_edgelist = pd.DataFrame(label_pairs_df.groupby(['e1','e2',edge_attr_name]\n",
    "                                                        ).size()).reset_index().sort(columns=0,ascending=False)\n",
    "    else:\n",
    "        label_edgelist = pd.DataFrame(label_pairs_df.groupby(['e1','e2']\n",
    "                                                        ).size()).reset_index().sort(columns=0,ascending=False)\n",
    "    \n",
    "    label_edgelist.rename(columns={0:'weight'},inplace=True)\n",
    "    \n",
    "    #Remove stop-words\n",
    "    if stop_words != None:\n",
    "        has_ttm =  [x in stop_words or y in stop_words for x,y in zip(label_edgelist['e1'],label_edgelist['e2'])]\n",
    "        label_edgelist = label_edgelist[-pd.Series(has_ttm)]\n",
    "    \n",
    "    #Create label graph\n",
    "    if type(edge_attr) != str:\n",
    "        label_graph = nx.from_pandas_dataframe(label_edgelist,'e1','e2',['weight',edge_attr_name])\n",
    "    else:\n",
    "        label_graph = nx.from_pandas_dataframe(label_edgelist,'e1','e2',['weight'])\n",
    "    \n",
    "    return(label_graph) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config_file = os.path.dirname(os.path.dirname(os.getcwd())) + '/config.json'\n",
    "\n",
    "#Load config file\n",
    "with open(config_file, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "#Create connection string\n",
    "conn_string = 'host={} dbname={} user={} password={}'.format(\n",
    "                    config.get(\"host\"),\n",
    "                    config.get(\"database\"),\n",
    "                    config.get(\"user\"),\n",
    "                    config.get(\"passw\"))\n",
    "\n",
    "#Create connection\n",
    "conn = psycopg2.connect(conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read the data\n",
    "#We'll read it in chunks\n",
    "chunk= 500\n",
    "\n",
    "#Create sql query string\n",
    "sql_query_str=\"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM\n",
    "        gtr.projects      \n",
    "\"\"\"\n",
    "\n",
    "#Read the data\n",
    "results = pd.read_sql(sql_query_str,con=conn,chunksize=chunk)\n",
    "\n",
    "#Create df from results\n",
    "projects_df = pd.DataFrame()\n",
    "for result in results:\n",
    "     projects_df = projects_df.append(result)\n",
    "        \n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "#reindex\n",
    "projects_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "#Select relevant columns and subset\n",
    "rel_vars = ['pkey','abstract_texts','grant_cats','href','identifiers',\n",
    "            'lead_org_dpts','links','research_subjects','research_topics','status']\n",
    "projects_rel_df = projects_df[rel_vars]\n",
    "\n",
    "#Extract the json elements form their 1 element-dicts\n",
    "projects_rel_df['research_subjects'] = projects_rel_df[\n",
    "    'research_subjects'].map(lambda x: x['researchSubject'])\n",
    "\n",
    "projects_rel_df['research_topics'] = projects_rel_df[\n",
    "    'research_topics'].map(lambda x: x['researchTopic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Focus analysis on research grant and fellowship projects\n",
    "#Subset\n",
    "projects_academic_df = projects_rel_df[[i in ['Research Grant','Fellowship'] for\n",
    "                                                 i in projects_rel_df.grant_cats]]\n",
    "        \n",
    "#Only consider projects with subject data (drop nas and )\n",
    "projects_w_subject_df = projects_academic_df[projects_academic_df.research_topics.notnull()]\n",
    "projects_w_subject_df = projects_academic_df[[len(i)>0 for i in\n",
    "                                             projects_academic_df.research_topics]]\n",
    "\n",
    "#Extract research subjects and research topics.\n",
    "projects_w_subject_df['subject_list'] = [[i['text'] for i in sublist] for sublist in \n",
    "                                       projects_w_subject_df['research_subjects']]\n",
    "projects_w_subject_df['topic_list'] = [[i['text'] for i in sublist] for sublist in \n",
    "                                       projects_w_subject_df['research_topics']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project_all_subjects = pd.Series([i for sublist in projects_w_subject_df['subject_list'] for\n",
    "                       i in sublist])\n",
    "projects_all_topics = pd.Series([i for sublist in projects_w_subject_df['topic_list'] for\n",
    "                       i in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:60: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:67: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "# Network Object Creation\n",
    "net = extract_network_from_corpus(projects_w_subject_df.topic_list, stop_words=\"Research approaches\")\n",
    "\n",
    "#Extract communities (discipline aggregates)\n",
    "topic_communities = community.best_partition(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The community objects are dicts where keys are subjects or topics, and values their communities\n",
    "#Use this group dict to relabel variables\n",
    "grouped_topics_dict = {\n",
    "    0: \"Engineering and Technology\",\n",
    "    1: \"Life Sciences\",\n",
    "    2: \"Physics\",\n",
    "    3: \"Arts and Humanities\",\n",
    "    4: \"Environmental Sciences\",\n",
    "    5: \"Mathematics and Computing\",\n",
    "    6: \"Social Sciences\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pkey</th>\n",
       "      <th>topic_list</th>\n",
       "      <th>aggregated_topics</th>\n",
       "      <th>topic_classified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37256</td>\n",
       "      <td>[Television HTC, Media &amp; Communication Studies]</td>\n",
       "      <td>[Engineering and Technology, Engineering and T...</td>\n",
       "      <td>Engineering and Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37258</td>\n",
       "      <td>[New &amp; Emerging Comp. Paradigms, Fundamentals ...</td>\n",
       "      <td>[Arts and Humanities, Arts and Humanities]</td>\n",
       "      <td>Arts and Humanities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>37262</td>\n",
       "      <td>[Ageing: chemistry/biochemistry, Animal &amp; huma...</td>\n",
       "      <td>[Life Sciences, Life Sciences, Life Sciences]</td>\n",
       "      <td>Life Sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>37264</td>\n",
       "      <td>[Agricultural systems, Land - Atmosphere Inter...</td>\n",
       "      <td>[Physics, Physics, Physics, Physics, Physics]</td>\n",
       "      <td>Physics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pkey                                         topic_list  \\\n",
       "1  37256    [Television HTC, Media & Communication Studies]   \n",
       "3  37258  [New & Emerging Comp. Paradigms, Fundamentals ...   \n",
       "7  37262  [Ageing: chemistry/biochemistry, Animal & huma...   \n",
       "9  37264  [Agricultural systems, Land - Atmosphere Inter...   \n",
       "\n",
       "                                   aggregated_topics  \\\n",
       "1  [Engineering and Technology, Engineering and T...   \n",
       "3         [Arts and Humanities, Arts and Humanities]   \n",
       "7      [Life Sciences, Life Sciences, Life Sciences]   \n",
       "9      [Physics, Physics, Physics, Physics, Physics]   \n",
       "\n",
       "             topic_classified  \n",
       "1  Engineering and Technology  \n",
       "3         Arts and Humanities  \n",
       "7               Life Sciences  \n",
       "9                     Physics  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lookup disciplines\n",
    "projects_w_subject_df['aggregated_topics'] = projects_w_subject_df.topic_list.map(\n",
    "    lambda x: [grouped_topics_dict[topic_communities[i]] for i in x])\n",
    "\n",
    "#If there are more than 2 disciplines, we call the project Mixed (a.k.a. multi/interdisciplinary)\n",
    "projects_w_subject_df['topic_classified'] = projects_w_subject_df['aggregated_topics'].map(\n",
    "    lambda x: list(set(x))[0] if len(set(x))==1 else \"Mixed\")\n",
    "\n",
    "#Check outputs\n",
    "projects_w_subject_df.ix[1:10, ['pkey', 'topic_list', 'aggregated_topics', 'topic_classified']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Higher Resolution Communities\n",
    "\n",
    "The community detection above gives just 6 communities. We would like to, if possible, identify higher resolution communities. We can try to accompish this using igraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import igraph\n",
    "import igraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the networkx object to gml\n",
    "write_network(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The following methodology comes from\n",
    "# http://stackoverflow.com/questions/25254151/using-igraph-in-python-for-community-detection-and-writing-community-number-for\n",
    "\n",
    "# Read it from igraph. Specify the graph you want here.\n",
    "g = igraph.read('../../data/networks/gtr_network_201608111227.gml')\n",
    "\n",
    "# calculate dendrogram\n",
    "edge_betweenness_dendrogram = g.community_edge_betweenness()\n",
    "\n",
    "# convert it into a flat clustering\n",
    "edge_betweenness_clusters = dendrogram.as_clustering()\n",
    "\n",
    "# get the membership vector\n",
    "edge_betweenness_membership = clusters.membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clustering with 607 elements and 54 clusters'"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_betweenness_clusters.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "walktrap_dendogram = g.community_walktrap()\n",
    "walktrap_clusters = walktrap_dendogram.as_clustering()\n",
    "walktrap_membership = walktrap_clusters.membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clustering with 607 elements and 7 clusters'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walktrap_clusters.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infomap_clusters = g.community_infomap()\n",
    "infomap_membership = infomap_clusters.membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clustering with 607 elements and 5 clusters'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infomap_clusters.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "igraph.Edge(<igraph.Graph object at 0x132eb2e58>, 1, {'weight': 2.0})"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.es[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eigen_clusters = g.community_leading_eigenvector()\n",
    "eigen_clusters_membership = eigen_clusters.membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clustering with 607 elements and 3 clusters'"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigen_clusters.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g.save('test.svg', format='svg', layout='kk', vertex_size=3, height=1000, width=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
